At firezone, we build secure remote access that scales, be it from your Android phone, MacOS computer or Linux server.
After several iterations, we’ve landed on a design for our connectivity library that we are extremely happy with.
It gives us blazingly fast and exhaustive tests, deep customisation and overall high assurance that it does what we want it to do.

This library - aptly named connlib is built in Rust and the design we are talking about is known as sans-IO.
Rust's premise of speed and memory-safety makes it a great choice for building network services.
Most parts of our Rust stack aren't particularly surprising:
We use the `tokio` runtime for asynchronous tasks, `tungstenite` for WebSockets, `boringtun` for the WireGuard implementation, `rustls` to encrypt traffic with the API, etc.
Yet, once you go beneath the surface of the library, you will discover something that is perhaps unusual:
There are almost no calls to `tokio::spawn`, all communication is multiplexed via a single UdpSocket and the same APIs appear to repeat themselves across various layers: `handle_timeout`, `poll_transmit`, `handle_input`, ...

These are the tell-tales of a sans-IO design:
instead of sending & receiving bytes via a socket in multiple places, our protocols are implemented as pure state machines.
Even time is abstracted away:
Every function that needs to know the current time receives an Instant parameter instead of calling `Instant::now` itself.
This pattern isn't something that we invented.
quinn makes use of it in its quinn-proto crate: https://github.com/quinn-rs/quinn/tree/main/quinn-proto.
There is also str0m, a sans-IO WebRTC implementation: https://github.com/algesten/str0m.

In this post, we'll go over what it means to build a sans-IO networking library, why we think it is a good idea and lastly, why Rust lends itself particularly well to this pattern.

## Rust's features

Before we dive into sans-IO itself, let me focus your attention on a few of Rust's features:

* ownership
* explicit mutable borrowing
* exhaustive matching
* powerful algebraic data types, especially enums

### Ownership & mutable borrowing

Rust forces us to declare, which component or function in our code owns a certain value, like a buffer.
Only the owner of a value can declare it mutable and thus either mutate itself or temporarily hand out mutable references to other functions.
As we will see, sans-IO driven designs heavily make use of this.

### Exhaustive matching and ADTs

The match keyword provides us with a compile-time checked way of handling the different variations of a value.
Combined with enums, this provides a powerful way of ensuring that all combinations are handled.
Enums lend themselves especially well in modelling state that can change at runtime.
For example, the following enum models the state of a Connection:

```rust
enum Connection {
    /// We are still running ICE to figure out, which socket to use to send data.
    Connecting {
        /// Socket addresses from which we might receive data (even before we are connected).
        possible_sockets: HashSet<SocketAddr>,
        /// Packets emitted by wireguard whilst are still running ICE.
        ///
        /// This can happen if the remote's WG session initiation arrives at our socket before we nominate it.
        /// A session initiation requires a response that we must not drop, otherwise the connection setup experiences unnecessary delays.
        buffered: RingBuffer<Vec<u8>>,
    },
    /// A socket has been nominated.
    Connected {
        /// Our nominated socket.
        peer_socket: PeerSocket,
        /// Other addresses that we might see traffic from (e.g.
        STUN messages during roaming).
        possible_sockets: HashSet<SocketAddr>,
    },
    /// The connection failed in an unrecoverable way and will be GC'd.
    Failed,
    /// The connection is idle and will be GC'd.
    Idle,
}
```

The key things about the above model are the invariants around which data is available in a particular state.
For example, buffered packets can only happen whilst we are Connecting.
Once we are Connected, there is no more need to buffer packets.
Similarly, once a Connection is failed, we no longer need to keep around the previously connected socket.

## Rust's async model & the "function colouring" problem

If you've been around the Rust space for a while, you will have probably heard about things like the "function colouring" problem.
In a nutshell, it expresses: async functions can only be called from other async functions.
As a result, an async function deep down in your stack "forces" every calling function to also become async in order to .await the inner function.
This can be problematic if the code you want to call isn't actually yours but a dependency that you are pulling in.

At the very bottom of each async call stack sits a Future that needs to suspend on something.
Usually, this is some form of IO, like writing to a socket, reading from a file, waiting for time to advance, etc.
The majority of async functions don't actually perform async work themselves.
Instead, they depend on an async primitive like a tokio::net::UdpSocket.
The code around those inner async functions would usually also work in a blocking context, yet the author happened to pick the async variant.
Here is an example of that:

```rust
// TODO: An example of writing a ping-pong function that uses tokio's Udp socket
```

This could be also be written using blocking IO from the standard library:

```rust
// TODO: An example of writing a ping-pong function that uses std's Udp socket
```

Notice how the code that decides what to send is exactly the same? The same goes for the error handling.
Both functions propagate the error up to the next layer.
There are lots of opinions out there as to what the "best" way of solving this duplication is.
sans-IO is one of them.

## Introducing sans-IO

The core idea of sans-IO is similar to the dependency inversion principle from the OOP world.
Whilst some OOP code out there might be a bit extreme in terms of following patterns (looking at you [`AbstractSingletonProxyFactoryBean`](https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/aop/framework/AbstractSingletonProxyFactoryBean.html)), I've found it helpful to explicitly spell some of these things out to really get to the bottom of a particular design.

The dependency inversion principle says that policies (what to do) should not depend on implementation details (how to do it).
Instead, both components should depend and communicate via abstractions.
In other words, the piece of code that decides to send a message on the network (i.e. the policy) should not depend on the code that actually sends the message (i.e. the implementation).
This is what is "wrong" with the above example:
We have a direct dependency on a UDP socket in the middle of our policy code, yet the policy code is usually the interesting one.
That is the one we want to test and perhaps share with others via libraries.

## Applying dependency inversion

How do we apply dependency inversion then?
We introduce abstractions!
When we call UdpSocket::send, what data are we actually passing?
The payload, a SocketAddr and - implicitly - the socket itself.
The socket can also be identified by means of a SocketAddr:
The one we bound to earlier in our application.
Let's package these three things up into an abstraction.
Meet Transmit:

```rust
pub struct Transmit {
    src: SocketAddr,
    dst: SocketAddr,
    payload: Vec<u8>
}
```

Anywhere where we'd like to send data over our UdpSocket, we can now emit a Transmit .
But that is only one half of the solution.
Where does the Transmit go? We need to execute this Transmit somewhere! This is the 2nd half of any sans-IO application.
There needs to be a component that actually causes all these side-effects.
Recall the definition of the dependency-inversion principle: Policies should not depend on implementations, instead both should depend on abstractions.
Transmit is our abstraction and we already know that we need to re-write our policy code to use it.
The actual implementation details, i.e. our UdpSocket also needs to be made aware of our new abstraction.
Luckily, this is pretty simple.
All we need is a function that takes a Transmit's dst and payload and passes it to the UdpSocket.

The avid reader will have already noticed that so far, we've glanced over an important detail:
What does the new code that "produces" a Transmit actually look like?
We can't simply make a Transmit and wait for it to magically wander through our application.
How do we connect those pieces of code together into something that actually works?

## Introducing state machines

If we think through our code in an abstract way, we may notice that it is actually a state machine:

```
// TODO: state diagram of our code
```

Without execute the side-effect of sending a message directly, we need to rewrite our code to resemble what it actually is: A state machine.
As we can see in our diagram, we have 3 states: Start, Sent, Received.
These are mutually-exclusive so we can model them as an enum:

```rust
enum State {
    Start,
    Sent,
    Received
}
```

Now, that we've laid out our data structure, let's add some functionality to it!
I'll call this function `handle_timeout` right away, the details as to why will follow in a bit.

```rust
// TODO: Code block with handle_timeout function.
```

In addition to `handle_timeout`, we will also need `handle_input`.
`handle_input` is like the inverse to Transmit.
We will use it to feed incoming data to our state machine, i.e. the result of `UdpSocket::read`.

Finally, we need a few auxiliary functions to actually construct a new instance of our state machine and to query things from it.

```rust
// TODO: Code block with new and poll_transmit
```

Putting all of this together, we now have a state machine that models the behaviour of our program without performing any IO itself.

## Abstracting time

TODO

## The premise of sans-IO

So far, all of this seems like a very excessive overhead for sending a few UDP packets back and forth.
Surely, the 10 line example above is preferable over this state machine!
The example might be, but recall the original problem we were discussing: function colouring.
In a code snippet without dependencies like the above example, using `async` seems like a no brainer and really easy.
The problem arises once you want to bring in dependencies.
Composing your functionality (i.e. policy) on top of those dependencies imposes their decisions around async vs blocking IO on you.
Libraries like `str0m` or `quinn-proto` which are written in the sans-IO way don't do that.
Instead, they are pure state machines and thus the decision about async vs blocking IO or which async runtime to use is deferred to the application.

### Easy composition

sans-IO code also composes extremely well.
Each "layer" depends on the same abstractions: `handle_timeout`, `handle_input`, `poll_transmit` & `poll_timeout`.
In the case of firezone, you can see this in the example of `snownet`, a library that combines ICE & wireguard and thereby exposes "magic" IP tunnels that work in any network setup to the rest of the application.

`snownet` builds on top of `str0m`, a sans-IO WebRTC library and `boringtun`, an (almost[^2]) sans-IO wireguard implementation.
We don’t need the majority of the WebRTC stack though.
The only thing we are interested in is the `IceAgent` which implements the ICE RFC (TODO link).
ICE implements a clever algorithm that ensures two agents, deployed into arbitrary network environments find the most optimal communication path to each other.
The result of ICE is a pair of socket addresses that we then use to perform a wireguard handshake.
Because `str0m` is built in a sans-IO fashion, only using the `IceAgent` part of it is shockingly trivial:
you simply only import that part of the dependency.

### Flexible APIs

sans-IO code needs to be "driven" by an eventloop of some sorts because it "just" expresses the state of the system but doesn’t cause any side-effects itself.
The eventloop is responsible for "querying" the state (like `poll_transmit`), executing it and also passing new input to the state machine (`handle_timeout` and `handle_input`).
To some people, this may appear as unnecessary boilerplate but it comes with a great benefit: flexibility.

* Want to make use of `sendmmsg` to reduce the number of syscalls when sending packets? Not a problem.
* TODO: More examples

Instead of seeing it as a burden to having to type some lines of code, we can see this as an opportunity to be able to tune our code to exactly what we want it to do.
This also makes maintenance easier for library authors: They can focus on implementing the core functionality of the library instead of how people want to use it.
A good example here is `str0m`’s stance on the gathering of candidates:
This is an IO concern and up to the application on how to achieve it.
`str0m` only provides an API to add such a candidate to the current state.

### Testing at the speed of light

sans-IO code is essentially side-effect free and thus lends itself extremely well for (unit) tests.
Due to sockets and time being abstracted away, it becomes a breeze to write tests that advance time by 5 minutes in an instant.
All we need to do is pass a modified `Instant` to our function and assert, how the code behaves.
To see a real world example of this, check out how we test that `snownet` closes idle connections after 5 minutes: https://github.com/firezone/firezone/blob/53557f46e452c0fe5195a4326873753a356c6005/rust/connlib/snownet/tests/lib.rs#L123-L127.

Similarly, actually sending data over a socket takes (a little bit of) time and more importantly, requires allocation of ports etc.
In a sans-IO world, "sending data" in a test is as simple as taking a `Transmit` from party B and calling `handle_input` on the state of party A.
No need to go through a network socket!

TODO: Should we directly talk about property-based testing here or just hint at it and make a dedicated post about it?

TODO: Write about no requirement for mocking, type parameters, trait objects etc?

## Rust's static analysis

How do Rust’s features that we discussed above fit into this? A sans-IO design only has "synchronous" APIs, i.e.
none of the functions on these state machines ever block on IO or time.
They are just data structures and Rust lends itself pretty well to modelling these:

* We can use `&mut` very liberally to express state that changes.
* We can use enums to express mutually-exclusive aspects.
* We can use exhaustive pattern matching to ensure no state combination is unhandled.

I would to place special emphasis on the usage of `&mut` here.
In Rust, `async` functions are just syntax sugar for a data structure that implements `Future`.
Spawning a `Future` into a runtime[^1] like `tokio` requires this data structure to be `'static` and therefore, it cannot contain any references, including `&mut`.
To mutate shared state in such a scenario, you basically have two options:

* Use reference-counted pointers and a mutex, i.e. `Arc<Mutex<T>>`
* Use "actors" and connect them via channels, i.e. spawn multiple tasks with loops that read and write to channels

Both of these options have a runtime overhead:
Locks can result in contention and sending messages through channels requires copying.
In addition, multiple tasks running inside a runtime operate in a non-deterministic order which can easily lead to race conditions and in the worst-case, dead-locks.
It appears that suddenly, we’ve arrived at a design that feels brittle, is prone to dead-locks and no longer employs zero-cost abstractions.
Avoiding all of these is one of the reasons we wanted to use Rust in the first-place!

[^1]: TODO Mention that it is isn’t true for single-thread runtimes but they are not common.
[^2]: TODO: Link to issue about impure time
